<h1 class="gap">0x11. Attention</h1>

<article id="description" class="gap formatted-content">
    <p><a href="https://ibb.co/hZbZHF3"><img src="https://i.ibb.co/8dqd2g1/4704cf0750335400050c494f69844150e6319d1b.jpg" alt="4704cf0750335400050c494f69844150e6319d1b" border="0"></a></p>

<h2>Resources:</h2>

<p><strong>Read or watch:</strong></p>

<ul>
<li><a href="/rltoken/KKW6LXmtOfzDrQCtX3trpg" title="How Does Attention Work in Encoder-Decoder Recurrent Neural Networks" target="_blank">How Does Attention Work in Encoder-Decoder Recurrent Neural Networks</a></li>
<li><a href="/rltoken/pw8xV6DMI1yMKY05Rzhq0g" title="Attention Model" target="_blank">Attention Model</a></li>
<li><a href="/rltoken/ju6UjnXhDuMFS6LzEIlj0Q" title="What is a Transformer?" target="_blank">What is a Transformer?</a></li>
<li><a href="/rltoken/ZhtVJj15dYPrQI8XRq-jCQ" title="How Transformers Work" target="_blank">How Transformers Work</a></li>
<li><a href="/rltoken/zeZrN58D0iOt161zLNqUMw" title="Transformer: A Novel Neural Network Architecture for Language Understanding" target="_blank">Transformer: A Novel Neural Network Architecture for Language Understanding</a></li>
<li><a href="/rltoken/01S-rlsZr6WfdsqMC2BEYQ" title="Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 – Transformers and Self-Attention" target="_blank">Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 – Transformers and Self-Attention</a></li>
<li><a href="/rltoken/AEZqvJXaWBEZVdGKy57yQw" title="(Transformer) Attention Is All You Need | AISC Foundational" target="_blank">(Transformer) Attention Is All You Need | AISC Foundational</a></li>
<li><a href="/rltoken/vdCwNWQ6pM10Rc2g2Nwo0A" title="Transformer Models in NLP" target="_blank">Transformer Models in NLP</a></li>
<li><a href="/rltoken/4o_KAqoKcamqDfuQ6OmI6g" title="Transformer model for language understanding" target="_blank">Transformer model for language understanding</a></li>
<li><a href="/rltoken/ukLu5CMMIoP_4Yt7MNY8UQ" title="Generative Modeling with Sparse Transformers" target="_blank">Generative Modeling with Sparse Transformers</a></li>
<li><a href="/rltoken/cXTOtTK9waaB-tM5C3EAMQ" title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li><a href="/rltoken/FoCSA7ZVrzZyTGUGIgEtPg" title="(BERT) Pretranied Deep Bidirectional Transformers for Language Understanding (algorithm) | TDLS" target="_blank">(BERT) Pretranied Deep Bidirectional Transformers for Language Understanding (algorithm) | TDLS</a></li>
</ul>

<p><strong>References:</strong></p>

<ul>
<li><a href="/rltoken/v3oiayVt7mUWFVHp-LsSlw" title="Sequence to Sequence Learning with Neural Networks (2014)" target="_blank">Sequence to Sequence Learning with Neural Networks (2014)</a></li>
<li><a href="/rltoken/cSywNMc0VkeDq4TZ4V8nEg" title="Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation (2014)" target="_blank">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation (2014)</a></li>
<li><a href="/rltoken/YlDIODUFbkYQbRL3a5CwEQ" title="Neural Machine Translation by Jointly Learning to Align and Translate" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="/rltoken/V29PxakOS66KNWeS016NAQ" title="Attention Is All You Need (2017)" target="_blank">Attention Is All You Need (2017)</a></li>
<li><a href="/rltoken/XEpXcqIg5l9DoS2-2E7InQ" title="tf.keras.layers.Embedding" target="_blank">tf.keras.layers.Embedding</a></li>
<li><a href="/rltoken/-qb21N0gxX5UmRub_bLasg" title="tf.keras.layers.LayerNormalization" target="_blank">tf.keras.layers.LayerNormalization</a></li>
<li><a href="/rltoken/fBJbaYK3mQliTYVmerk8Gw" title="Improving Language Understanding by Generative Pre-Training (2018)" target="_blank">Improving Language Understanding by Generative Pre-Training (2018)</a></li>
<li><a href="/rltoken/Xo7RMyK_8WiLXaerfTHoWw" title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)</a></li>
<li><a href="/rltoken/hj1iAmCUDCnH72SGnTxsXA" title="SQuAD 2.0" target="_blank">SQuAD 2.0</a></li>
<li><a href="/rltoken/2gCjBpi3CDZ25VyRnwy3ow" title="Know What You Don’t Know: Unanswerable Questions for SQuAD (2018)" target="_blank">Know What You Don’t Know: Unanswerable Questions for SQuAD (2018)</a></li>
<li><a href="/rltoken/XQMKBVpVRJ7PH_Fam-mYkw" title="GLUE Benchmark" target="_blank">GLUE Benchmark</a></li>
<li><a href="/rltoken/gc0N-1a5GhwB6r8i4-PJ4A" title="GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding (2019)" target="_blank">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding (2019)</a></li>
</ul>

<p><strong>More recent papers in NLP:</strong></p>

<ul>
<li><a href="/rltoken/Vr6j7CGFKtuK7IetaIeMcw" title="Generating Long Sequences with Sparse Transformers (2019)" target="_blank">Generating Long Sequences with Sparse Transformers (2019)</a></li>
<li><a href="/rltoken/kEkphkks_kGLj3XuXBMCAA" title="Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019)" target="_blank">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019)</a></li>
<li><a href="/rltoken/THgst72PaoA0cYrpfOQWDA" title="XLNet: Generalized Autoregressive Pretraining for Language Understanding (2019)" target="_blank">XLNet: Generalized Autoregressive Pretraining for Language Understanding (2019)</a></li>
<li><a href="/rltoken/C2rjrR_0Dq0_XU8Bu9INbQ" title="Language Models are Unsupervised Multitask Learners (GPT-2, 2019)" target="_blank">Language Models are Unsupervised Multitask Learners (GPT-2, 2019)</a></li>
<li><a href="/rltoken/2y-mtFWYnQ8QbtMij0PoWA" title="Language Models are Few-Shot Learners (GPT-3, 2020)" target="_blank">Language Models are Few-Shot Learners (GPT-3, 2020)</a></li>
<li><a href="/rltoken/u6oVFipCalDn-X1QTwVsPA" title="ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations (2020)" target="_blank">ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations (2020)</a></li>
</ul>

<p>To keep up with the newest papers and their code bases go to <a href="/rltoken/3VaoLjK3QGn7jJbnU6MpKQ" title="paperswithcode.com" target="_blank">paperswithcode.com</a>. For example, check out the <a href="/rltoken/v3qkGtmWqnn_M3JgkqukYQ" title="raked list of state of the art models for Language Modelling on Penn Treebank" target="_blank">raked list of state of the art models for Language Modelling on Penn Treebank</a>.</p>

<h2>Learning Objectives</h2>

<p>At the end of this project, you are expected to be able to <a href="/rltoken/rm_YJ9QKGU1TCkyffIRbDg" title="explain to anyone" target="_blank">explain to anyone</a>, <strong>without the help of Google</strong>:</p>

<h3>General</h3>

<ul>
<li>What is the attention mechanism?</li>
<li>How to apply attention to RNNs</li>
<li>What is a transformer?</li>
<li>How to create an encoder-decoder transformer model</li>
<li>What is GPT? </li>
<li>What is BERT?</li>
<li>What is self-supervised learning?</li>
<li>How to use BERT for specific NLP tasks</li>
<li>What is SQuAD? GLUE?</li>
</ul>

<h2>Requirements</h2>

<h3>General</h3>

<ul>
<li>Allowed editors: <code>vi</code>, <code>vim</code>, <code>emacs</code></li>
<li>All your files will be interpreted/compiled on Ubuntu 16.04 LTS using <code>python3</code> (version 3.5)</li>
<li>Your files will be executed with <code>numpy</code> (version 1.16) and <code>tensorflow</code> (version 1.15)</li>
<li>All your files should end with a new line</li>
<li>The first line of all your files should be exactly <code>#!/usr/bin/env python3</code></li>
<li>All of your files must be executable</li>
<li>A <code>README.md</code> file, at the root of the folder of the project, is mandatory</li>
<li>Your code should follow the <code>pycodestyle</code> style (version 2.4)</li>
<li>All your modules should have documentation (<code>python3 -c 'print(__import__("my_module").__doc__)'</code>)</li>
<li>All your classes should have documentation (<code>python3 -c 'print(__import__("my_module").MyClass.__doc__)'</code>)</li>
<li>All your functions (inside and outside a class) should have documentation (<code>python3 -c 'print(__import__("my_module").my_function.__doc__)'</code> and <code>python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)'</code>)</li>
<li>Unless otherwise stated, you cannot import any module except <code>import tensorflow as tf</code></li>
</ul>

<h2>Update Tensorflow to 1.15</h2>

<p>In order to complete the following tasks, you will need to update <code>tensorflow</code> to version 1.15, which will also update <code>numpy</code> to version 1.16</p>

<pre><code>pip install --user tensorflow==1.15
</code></pre>

  </article>

## Author
* **Samir millan** - [Gaspela04](https://github.com/Gaspela04)